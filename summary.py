import os
import pdfplumber
from docx import Document
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from transformers import T5Tokenizer, AutoModelForSeq2SeqLM

# --- âœ… à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸ªà¸³à¸«à¸£à¸±à¸šà¸„à¹‰à¸™à¸«à¸²à¹à¸¥à¸°à¸ªà¸£à¸¸à¸› ---
search_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # à¸ªà¸³à¸«à¸£à¸±à¸šà¸„à¹‰à¸™à¸«à¸²
summary_tokenizer = T5Tokenizer.from_pretrained("csebuetnlp/mT5_multilingual_XLSum")  # à¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸£à¸¸à¸›
summary_model = AutoModelForSeq2SeqLM.from_pretrained("csebuetnlp/mT5_multilingual_XLSum")

# --- ğŸ“¥ à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œà¹€à¸­à¸à¸ªà¸²à¸£ ---
def load_txt_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

def load_pdf_file(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

def load_docx_file(file_path):
    doc = Document(file_path)
    return "\n".join([para.text for para in doc.paragraphs])

# --- ğŸ“‚ à¹‚à¸«à¸¥à¸”à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸ˆà¸²à¸à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ ---
def load_documents(folder_path):
    documents, filenames = [], []
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        if filename.endswith('.txt'):
            content = load_txt_file(file_path)
        elif filename.endswith('.pdf'):
            content = load_pdf_file(file_path)
        elif filename.endswith('.docx'):
            content = load_docx_file(file_path)
        else:
            continue
        documents.append(content)
        filenames.append(filename)
    return documents, filenames

# --- ğŸ“ à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸ªà¸£à¸¸à¸›à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ ---
def summarize_text(text, max_length=100, min_length=30):
    prompt = "summarize: " + text
    inputs = summary_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    summary_ids = summary_model.generate(
        inputs["input_ids"],
        max_length=max_length,
        min_length=min_length,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True
    )
    return summary_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# --- ğŸ” à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸„à¹‰à¸™à¸«à¸²à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡ ---
def search_documents(query, documents, filenames):
    query_embedding = search_model.encode([query], convert_to_numpy=True)
    doc_embeddings = search_model.encode(documents, convert_to_numpy=True)
    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]
    ranked_indices = np.argsort(similarities)[::-1]
    ranked_documents = [documents[i] for i in ranked_indices]
    ranked_filenames = [filenames[i] for i in ranked_indices]
    ranked_scores = [similarities[i] for i in ranked_indices]
    return ranked_documents, ranked_filenames, ranked_scores

# --- ğŸš€ à¹€à¸£à¸´à¹ˆà¸¡à¸—à¸³à¸‡à¸²à¸™ ---
folder_path = r'C:\Users\Yok\Desktop\nlp\file_test'  # ğŸ”” à¹à¸à¹‰à¹€à¸›à¹‡à¸™ path à¸‚à¸­à¸‡à¸„à¸¸à¸“
documents, filenames = load_documents(folder_path)

query = "robot"  # ğŸ” à¸„à¸³à¸„à¹‰à¸™à¸«à¸²
ranked_documents, ranked_filenames, ranked_scores = search_documents(query, documents, filenames)

# --- ğŸ“¢ à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸à¸£à¹‰à¸­à¸¡à¸ªà¸£à¸¸à¸› ---
print("\nğŸ” Top 5 à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡:")
for i in range(min(5, len(ranked_filenames))):
    print(f"\n{i+1}. ğŸ“„ à¹„à¸Ÿà¸¥à¹Œ: {ranked_filenames[i]}")
    print(f"âœ… à¸„à¸§à¸²à¸¡à¸„à¸¥à¹‰à¸²à¸¢à¸„à¸¥à¸¶à¸‡: {ranked_scores[i]:.2f}")
    print(f"ğŸ“š à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸šà¸²à¸‡à¸ªà¹ˆà¸§à¸™: {ranked_documents[i][:300]}...")  # à¹à¸ªà¸”à¸‡ 300 à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£à¹à¸£à¸

    summary = summarize_text(ranked_documents[i])  # ğŸ“ à¸ªà¸£à¸¸à¸›à¹€à¸™à¸·à¹‰à¸­à¸«à¸²
    print(f"ğŸ“ à¸ªà¸£à¸¸à¸›à¹€à¸™à¸·à¹‰à¸­à¸«à¸²: {summary}")
